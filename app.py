# import streamlit as st
# import requests
# import json
# from langchain.chat_models.base import BaseChatModel
# from langchain.prompts.chat import (
#     ChatPromptTemplate,
#     SystemMessagePromptTemplate,
#     HumanMessagePromptTemplate
# )
# from langchain.chains import SequentialChain, LLMChain
# from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage, ChatResult, ChatGeneration
# import logging
# from typing import List, Optional

# # Definindo a classe do modelo de chat
# class OllamaChat(BaseChatModel):
#     url: str
#     headers: dict
#     model_name: str

#     def get_prompt(self, messages: List[BaseMessage]) -> str:
#         prompt = f"{messages[0].content} "
#         for i, message in enumerate(messages[1:]):
#             if isinstance(message, HumanMessage):
#                 prompt += f"USU√ÅRIO: {message.content} "
#             elif isinstance(message, AIMessage):
#                 prompt += f"ASSISTENTE: {message.content}</s>"
#         prompt += f"ASSISTENTE:"
#         return prompt

#     def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> ChatResult:
#         prompt = self.get_prompt(messages)
#         payload = {
#             "model": self.model_name,
#             "prompt": prompt
#         }
#         responses = []
#         try:
#             response = requests.post(self.url, headers=self.headers, data=json.dumps(payload))
            
#             raw_response = response.content.decode('utf-8')

#             # Separe os diferentes objetos JSON
#             json_objects = raw_response.split('\n')

#             # Inicialize uma lista para armazenar as respostas
#             responses = []
            
#             for obj in json_objects:
#                 if obj.strip():  # Certifique-se de ignorar linhas vazias
#                     try:
#                         parsed_obj = json.loads(obj)
#                         responses.append(parsed_obj)
#                     except json.JSONDecodeError as e:
#                         print(f"Erro ao decodificar JSON: {e}")

#             # Combine as respostas para formar a resposta final
#             final_response = ''.join([resp['response'] for resp in responses if 'response' in resp])
#             generated_text = final_response.strip()
            
#         except requests.RequestException as e:
#             generated_text = f"Erro na solicita√ß√£o: {str(e)}"

#         ai_message = AIMessage(content=generated_text)
#         chat_result = ChatResult(generations=[ChatGeneration(message=ai_message)])
#         return chat_result

#     @property
#     def _llm_type(self) -> str:
#         return self.model_name

#     def _agenerate(self):
#         return None

# # Inicializando o modelo
# url = "http://localhost:11434/api/generate"
# headers = {"Content-Type": "application/json"}
# model_name = "llama3"

# chat = OllamaChat(url=url, headers=headers, model_name=model_name)

# # Definindo o template para o assistente geral
# class GeneralEducationTemplate:
#     def __init__(self):
#         self.system_template = """
#         Voc√™ √© um professor altamente qualificado com experi√™ncia em uma ampla gama de t√≥picos acad√™micos e educacionais. Sua tarefa √© responder a perguntas gerais dos usu√°rios, fornecendo explica√ß√µes claras e detalhadas. Al√©m disso, voc√™ deve criar quizzes interativos para ajudar os usu√°rios a testar e aprofundar seu conhecimento sobre qualquer assunto que eles desejarem aprender.
#         Voc√™ deve lembrar as perguntas feitas para responder quando o usu√°rio perguntar. Deve responder sempre no mesmo idioma que for realizado a pergunta.

#         **Diretrizes para suas respostas:**
#         - **Clareza**: Sempre procure explicar os conceitos de forma clara e compreens√≠vel.
#         - **Detalhamento**: Forne√ßa detalhes suficientes para cobrir o t√≥pico solicitado sem sobrecarregar o usu√°rio com informa√ß√µes excessivas.
#         - **Exemplos**: Sempre que poss√≠vel, inclua exemplos pr√°ticos para ilustrar os conceitos.
#         - **Interatividade**: Quando criar quizzes, ofere√ßa perguntas que desafiem o conhecimento do usu√°rio e forne√ßa feedback sobre as respostas corretas e incorretas.
#         - **Tom**: Mantenha um tom amig√°vel e encorajador, como se voc√™ estivesse conversando diretamente com o usu√°rio.

#         Por exemplo, se um usu√°rio pergunta sobre "como funciona a fotoss√≠ntese", sua resposta deve explicar o processo em termos simples, talvez come√ßando com uma breve defini√ß√£o, seguida pelos principais passos da fotoss√≠ntese, e terminando com um exemplo de como as plantas utilizam este processo para crescer. Se solicitado, voc√™ tamb√©m pode criar um pequeno quiz para testar a compreens√£o do usu√°rio sobre o tema.
#         """
        
#         self.human_template = """
#         ####{request}####
#         """
#         self.system_message_prompt = SystemMessagePromptTemplate.from_template(self.system_template)
#         self.human_message_prompt = HumanMessagePromptTemplate.from_template(self.human_template, input_variables=["request"])
#         self.chat_prompt = ChatPromptTemplate.from_messages([self.system_message_prompt, self.human_message_prompt])

# class Agent:
#     def __init__(self, chat_model, verbose=True):
#         self.logger = logging.getLogger(__name__)
#         if verbose:
#             self.logger.setLevel(logging.INFO)
#         self.chat_model = chat_model
#         self.verbose = verbose

#     def get_response(self, request):
#         template = GeneralEducationTemplate()
#         education_chain = LLMChain(
#             llm=self.chat_model,
#             prompt=template.chat_prompt,
#             verbose=self.verbose,
#             output_key='response'
#         )
#         overall_chain = SequentialChain(
#             chains=[education_chain],
#             input_variables=["request"],
#             output_variables=["response"],
#             verbose=self.verbose
#         )
#         return overall_chain({"request": request}, return_only_outputs=True)

# # Inicializando o agente geral
# my_agent = Agent(chat)

# # Streamlit App
# st.set_page_config(page_title="App de Aux√≠lio em Aprendizagem")
# st.title("App de Aux√≠lio em Aprendizagem üìö")

# # Sidebar for parameters
# st.sidebar.header("Par√¢metros ‚öôÔ∏è")
# temperature = st.sidebar.slider("Temperatura: Controla a aleatoriedade das respostas geradas pelo modelo!", min_value=0.01, max_value=1.00, value=0.10, step=0.01)
# max_length = st.sidebar.slider("Comprimento m√°ximo: Define o comprimento m√°ximo da resposta gerada.", min_value=32, max_value=128, value=120, step=1)

# # Clear chat history button
# if st.sidebar.button("üóëÔ∏è Clear Chat History"):
#     st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# # Main chat interface
# if 'chat_history' not in st.session_state:
#     st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# # Display chat history
# for chat in st.session_state['chat_history']:
#     with st.chat_message(chat["role"]):
#         st.markdown(chat["content"])

# # Input for user message
# if prompt := st.chat_input("Digite sua pergunta ou t√≥pico de interesse:", disabled=not st.sidebar.button):
#     st.session_state['chat_history'].append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # Get response from agent
#     result = my_agent.get_response(prompt)
#     response = result["response"]

#     # Add agent response to chat history
#     st.session_state['chat_history'].append({"role": "assistant", "content": response})
#     with st.chat_message("assistant"):
#         st.markdown(response)


# import streamlit as st
# import requests
# import json
# from langchain.chat_models.base import BaseChatModel
# from langchain.prompts.chat import (
#     ChatPromptTemplate,
#     SystemMessagePromptTemplate,
#     HumanMessagePromptTemplate
# )
# from langchain.chains import SequentialChain, LLMChain
# from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage, ChatResult, ChatGeneration
# import logging
# from typing import List, Optional

# # Definindo a classe do modelo de chat
# class OllamaChat(BaseChatModel):
#     url: str
#     headers: dict
#     model_name: str

#     def get_prompt(self, messages: List[BaseMessage]) -> str:
#         prompt = f"{messages[0].content} "
#         for i, message in enumerate(messages[1:]):
#             if isinstance(message, HumanMessage):
#                 prompt += f"USU√ÅRIO: {message.content} "
#             elif isinstance(message, AIMessage):
#                 prompt += f"ASSISTENTE: {message.content}</s>"
#         prompt += f"ASSISTENTE:"
#         return prompt

#     def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> ChatResult:
#         prompt = self.get_prompt(messages)
#         payload = {
#             "model": self.model_name,
#             "prompt": prompt
#         }
#         responses = []
#         try:
#             response = requests.post(self.url, headers=self.headers, data=json.dumps(payload), stream=True)

#             generated_text = ""
#             for chunk in response.iter_lines():
#                 if chunk:
#                     decoded_chunk = chunk.decode('utf-8')
#                     try:
#                         parsed_obj = json.loads(decoded_chunk)
#                         if 'response' in parsed_obj:
#                             response_part = parsed_obj['response']
#                             generated_text += response_part
#                             # Atualiza o carregamento no Streamlit
#                             with st.spinner("Aguardando resposta do modelo..."):
#                                 st.markdown(generated_text.strip())
#                     except json.JSONDecodeError as e:
#                         print(f"Erro ao decodificar JSON: {e}")
            
#             generated_text = generated_text.strip()
            
#         except requests.RequestException as e:
#             generated_text = f"Erro na solicita√ß√£o: {str(e)}"

#         ai_message = AIMessage(content=generated_text)
#         chat_result = ChatResult(generations=[ChatGeneration(message=ai_message)])
#         return chat_result

#     @property
#     def _llm_type(self) -> str:
#         return self.model_name

#     def _agenerate(self):
#         return None

# # Inicializando o modelo
# url = "http://localhost:11434/api/generate"
# headers = {"Content-Type": "application/json"}
# model_name = "llama3"

# chat = OllamaChat(url=url, headers=headers, model_name=model_name)

# # Definindo o template para o assistente geral
# class GeneralEducationTemplate:
#     def __init__(self):
#         self.system_template = """
#         Voc√™ √© um professor altamente qualificado com experi√™ncia em uma ampla gama de t√≥picos acad√™micos e educacionais. Sua tarefa √© responder a perguntas gerais dos usu√°rios, fornecendo explica√ß√µes claras e detalhadas. Al√©m disso, voc√™ deve criar quizzes interativos para ajudar os usu√°rios a testar e aprofundar seu conhecimento sobre qualquer assunto que eles desejarem aprender.
#         Voc√™ deve lembrar as perguntas feitas para responder quando o usu√°rio perguntar. Deve responder sempre no mesmo idioma que for realizado a pergunta.

#         **Diretrizes para suas respostas:**
#         - **Clareza**: Sempre procure explicar os conceitos de forma clara e compreens√≠vel.
#         - **Detalhamento**: Forne√ßa detalhes suficientes para cobrir o t√≥pico solicitado sem sobrecarregar o usu√°rio com informa√ß√µes excessivas.
#         - **Exemplos**: Sempre que poss√≠vel, inclua exemplos pr√°ticos para ilustrar os conceitos.
#         - **Interatividade**: Quando criar quizzes, ofere√ßa perguntas que desafiem o conhecimento do usu√°rio e forne√ßa feedback sobre as respostas corretas e incorretas.
#         - **Tom**: Mantenha um tom amig√°vel e encorajador, como se voc√™ estivesse conversando diretamente com o usu√°rio.

#         Por exemplo, se um usu√°rio pergunta sobre "como funciona a fotoss√≠ntese", sua resposta deve explicar o processo em termos simples, talvez come√ßando com uma breve defini√ß√£o, seguida pelos principais passos da fotoss√≠ntese, e terminando com um exemplo de como as plantas utilizam este processo para crescer. Se solicitado, voc√™ tamb√©m pode criar um pequeno quiz para testar a compreens√£o do usu√°rio sobre o tema.
#         """
        
#         self.human_template = """
#         ####{request}####
#         """
#         self.system_message_prompt = SystemMessagePromptTemplate.from_template(self.system_template)
#         self.human_message_prompt = HumanMessagePromptTemplate.from_template(self.human_template, input_variables=["request"])
#         self.chat_prompt = ChatPromptTemplate.from_messages([self.system_message_prompt, self.human_message_prompt])

# class Agent:
#     def __init__(self, chat_model, verbose=True):
#         self.logger = logging.getLogger(__name__)
#         if verbose:
#             self.logger.setLevel(logging.INFO)
#         self.chat_model = chat_model
#         self.verbose = verbose

#     def get_response(self, request):
#         template = GeneralEducationTemplate()
#         education_chain = LLMChain(
#             llm=self.chat_model,
#             prompt=template.chat_prompt,
#             verbose=self.verbose,
#             output_key='response'
#         )
#         overall_chain = SequentialChain(
#             chains=[education_chain],
#             input_variables=["request"],
#             output_variables=["response"],
#             verbose=self.verbose
#         )
#         return overall_chain({"request": request}, return_only_outputs=True)

# # Inicializando o agente geral
# my_agent = Agent(chat)

# # Streamlit App
# st.set_page_config(page_title="App de Aux√≠lio em Aprendizagem")
# st.title("App de Aux√≠lio em Aprendizagem üìö")

# # Sidebar for parameters
# st.sidebar.header("Par√¢metros ‚öôÔ∏è")
# temperature = st.sidebar.slider("Temperatura: Controla a aleatoriedade das respostas geradas pelo modelo!", min_value=0.01, max_value=1.00, value=0.10, step=0.01)
# max_length = st.sidebar.slider("Comprimento m√°ximo: Define o comprimento m√°ximo da resposta gerada.", min_value=32, max_value=128, value=120, step=1)

# # Clear chat history button
# if st.sidebar.button("üóëÔ∏è Clear Chat History"):
#     st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# # Main chat interface
# if 'chat_history' not in st.session_state:
#     st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# # Display chat history
# for chat in st.session_state['chat_history']:
#     with st.chat_message(chat["role"]):
#         st.markdown(chat["content"])

# # Input for user message
# if prompt := st.chat_input("Digite sua pergunta ou t√≥pico de interesse:", disabled=not st.sidebar.button):
#     st.session_state['chat_history'].append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # Get response from agent
#     result = my_agent.get_response(prompt)
#     response = result["response"]

#     # Add agent response to chat history
#     st.session_state['chat_history'].append({"role": "assistant", "content": response})
#     with st.chat_message("assistant"):
#         st.markdown(response)


# import streamlit as st
# import requests
# import json
# from langchain.chat_models.base import BaseChatModel
# from langchain.prompts.chat import (
#     ChatPromptTemplate,
#     SystemMessagePromptTemplate,
#     HumanMessagePromptTemplate
# )
# from langchain.chains import SequentialChain, LLMChain
# from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage, ChatResult, ChatGeneration
# import logging
# from typing import List, Optional

# # Definindo a classe do modelo de chat
# class OllamaChat(BaseChatModel):
#     url: str
#     headers: dict
#     model_name: str

#     def get_prompt(self, messages: List[BaseMessage]) -> str:
#         prompt = f"{messages[0].content} "
#         for i, message in enumerate(messages[1:]):
#             if isinstance(message, HumanMessage):
#                 prompt += f"USU√ÅRIO: {message.content} "
#             elif isinstance(message, AIMessage):
#                 prompt += f"ASSISTENTE: {message.content}</s>"
#         prompt += f"ASSISTENTE:"
#         return prompt

#     def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> ChatResult:
#         prompt = self.get_prompt(messages)
#         payload = {
#             "model": self.model_name,
#             "prompt": prompt
#         }
#         responses = []
#         try:
#             response = requests.post(self.url, headers=self.headers, data=json.dumps(payload), stream=True)
            
#             generated_text = ""
#             full_response = ""
#             placeholder = st.empty()  # Espa√ßo reservado para atualizar dinamicamente a resposta

#             for chunk in response.iter_lines():
#                 if chunk:
#                     decoded_chunk = chunk.decode('utf-8')
#                     try:
#                         parsed_obj = json.loads(decoded_chunk)
#                         if 'response' in parsed_obj:
#                             response_part = parsed_obj['response']
#                             full_response += response_part
#                             placeholder.markdown(full_response.strip())  # Atualiza o espa√ßo reservado
#                     except json.JSONDecodeError as e:
#                         print(f"Erro ao decodificar JSON: {e}")

#             generated_text = full_response.strip()
            
#         except requests.RequestException as e:
#             generated_text = f"Erro na solicita√ß√£o: {str(e)}"

#         ai_message = AIMessage(content=generated_text)
#         chat_result = ChatResult(generations=[ChatGeneration(message=ai_message)])
#         return chat_result

#     @property
#     def _llm_type(self) -> str:
#         return self.model_name

#     def _agenerate(self):
#         return None

# # Inicializando o modelo
# url = "http://localhost:11434/api/generate"
# headers = {"Content-Type": "application/json"}
# model_name = "llama3"

# chat = OllamaChat(url=url, headers=headers, model_name=model_name)

# # Definindo o template para o assistente geral
# class GeneralEducationTemplate:
#     def __init__(self):
#         self.system_template = """
#         Voc√™ √© um professor altamente qualificado com experi√™ncia em uma ampla gama de t√≥picos acad√™micos e educacionais. Sua tarefa √© responder a perguntas gerais dos usu√°rios, fornecendo explica√ß√µes claras e detalhadas. Al√©m disso, voc√™ deve criar quizzes interativos para ajudar os usu√°rios a testar e aprofundar seu conhecimento sobre qualquer assunto que eles desejarem aprender.
#         Voc√™ deve lembrar as perguntas feitas para responder quando o usu√°rio perguntar. Deve responder sempre no mesmo idioma que for realizado a pergunta.

#         **Diretrizes para suas respostas:**
#         - **Clareza**: Sempre procure explicar os conceitos de forma clara e compreens√≠vel.
#         - **Detalhamento**: Forne√ßa detalhes suficientes para cobrir o t√≥pico solicitado sem sobrecarregar o usu√°rio com informa√ß√µes excessivas.
#         - **Exemplos**: Sempre que poss√≠vel, inclua exemplos pr√°ticos para ilustrar os conceitos.
#         - **Interatividade**: Quando criar quizzes, ofere√ßa perguntas que desafiem o conhecimento do usu√°rio e forne√ßa feedback sobre as respostas corretas e incorretas.
#         - **Tom**: Mantenha um tom amig√°vel e encorajador, como se voc√™ estivesse conversando diretamente com o usu√°rio.

#         Por exemplo, se um usu√°rio pergunta sobre "como funciona a fotoss√≠ntese", sua resposta deve explicar o processo em termos simples, talvez come√ßando com uma breve defini√ß√£o, seguida pelos principais passos da fotoss√≠ntese, e terminando com um exemplo de como as plantas utilizam este processo para crescer. Se solicitado, voc√™ tamb√©m pode criar um pequeno quiz para testar a compreens√£o do usu√°rio sobre o tema.
#         """
        
#         self.human_template = """
#         ####{request}####
#         """
#         self.system_message_prompt = SystemMessagePromptTemplate.from_template(self.system_template)
#         self.human_message_prompt = HumanMessagePromptTemplate.from_template(self.human_template, input_variables=["request"])
#         self.chat_prompt = ChatPromptTemplate.from_messages([self.system_message_prompt, self.human_message_prompt])

# class Agent:
#     def __init__(self, chat_model, verbose=True):
#         self.logger = logging.getLogger(__name__)
#         if verbose:
#             self.logger.setLevel(logging.INFO)
#         self.chat_model = chat_model
#         self.verbose = verbose

#     def get_response(self, request):
#         template = GeneralEducationTemplate()
#         education_chain = LLMChain(
#             llm=self.chat_model,
#             prompt=template.chat_prompt,
#             verbose=self.verbose,
#             output_key='response'
#         )
#         overall_chain = SequentialChain(
#             chains=[education_chain],
#             input_variables=["request"],
#             output_variables=["response"],
#             verbose=self.verbose
#         )
#         return overall_chain({"request": request}, return_only_outputs=True)

# # Inicializando o agente geral
# my_agent = Agent(chat)

# # Streamlit App
# st.set_page_config(page_title="App de Aux√≠lio em Aprendizagem")
# st.title("App de Aux√≠lio em Aprendizagem üìö")

# # Sidebar for parameters
# st.sidebar.header("Par√¢metros ‚öôÔ∏è")
# temperature = st.sidebar.slider("Temperatura: Controla a aleatoriedade das respostas geradas pelo modelo!", min_value=0.01, max_value=1.00, value=0.10, step=0.01)
# max_length = st.sidebar.slider("Comprimento m√°ximo: Define o comprimento m√°ximo da resposta gerada.", min_value=32, max_value=128, value=120, step=1)

# # Clear chat history button
# if st.sidebar.button("üóëÔ∏è Clear Chat History"):
#     st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# # Main chat interface
# if 'chat_history' not in st.session_state:
#     st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# # Display chat history
# for chat in st.session_state['chat_history']:
#     with st.chat_message(chat["role"]):
#         st.markdown(chat["content"])

# # Input for user message
# if prompt := st.chat_input("Digite sua pergunta ou t√≥pico de interesse:", disabled=not st.sidebar.button):
#     st.session_state['chat_history'].append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # Get response from agent
#     result = my_agent.get_response(prompt)
#     response = result["response"]

#     # Add agent response to chat history
#     st.session_state['chat_history'].append({"role": "assistant", "content": response})
#     with st.chat_message("assistant"):
#         st.markdown(response)

import streamlit as st
import requests
import json
import time
from langchain.chat_models.base import BaseChatModel
from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import SequentialChain, LLMChain
from langchain.schema import BaseMessage, AIMessage, HumanMessage
import logging
from typing import List, Optional

# Definindo a classe do modelo de chat
class OllamaChat(BaseChatModel):
    url: str
    headers: dict
    model_name: str

    def get_prompt(self, messages: List[BaseMessage]) -> str:
        prompt = f"{messages[0].content} "
        for i, message in enumerate(messages[1:]):
            if isinstance(message, HumanMessage):
                prompt += f"USU√ÅRIO: {message.content} "
            elif isinstance(message, AIMessage):
                prompt += f"ASSISTENTE: {message.content}</s>"
        prompt += f"ASSISTENTE:"
        return prompt

    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> str:
        prompt = self.get_prompt(messages)
        payload = {
            "model": self.model_name,
            "prompt": prompt
        }
        responses = []
        try:
            response = requests.post(self.url, headers=self.headers, data=json.dumps(payload), stream=True)
            
            generated_text = ""
            placeholder = st.empty()  # Espa√ßo reservado para atualizar dinamicamente a resposta

            for chunk in response.iter_lines():
                if chunk:
                    decoded_chunk = chunk.decode('utf-8')
                    try:
                        parsed_obj = json.loads(decoded_chunk)
                        if 'response' in parsed_obj:
                            response_part = parsed_obj['response']
                            generated_text += response_part
                            # Atualiza o espa√ßo reservado com um pequeno atraso para simular digita√ß√£o
                            placeholder.markdown(generated_text.strip())
                            time.sleep(0.05)  # Ajuste o tempo conforme necess√°rio
                    except json.JSONDecodeError as e:
                        print(f"Erro ao decodificar JSON: {e}")

            generated_text = generated_text.strip()
            
        except requests.RequestException as e:
            generated_text = f"Erro na solicita√ß√£o: {str(e)}"

        return generated_text

    @property
    def _llm_type(self) -> str:
        return self.model_name

    def _agenerate(self):
        return None

# Inicializando o modelo
url = "http://localhost:11434/api/generate"
headers = {"Content-Type": "application/json"}
model_name = "llama3"

chat = OllamaChat(url=url, headers=headers, model_name=model_name)

# Definindo o template para o assistente geral
class GeneralEducationTemplate:
    def __init__(self):
        self.system_template = """
        Voc√™ √© um professor altamente qualificado com experi√™ncia em uma ampla gama de t√≥picos acad√™micos e educacionais. Sua tarefa √© responder a perguntas gerais dos usu√°rios, fornecendo explica√ß√µes claras e detalhadas. Al√©m disso, voc√™ deve criar quizzes interativos para ajudar os usu√°rios a testar e aprofundar seu conhecimento sobre qualquer assunto que eles desejarem aprender.
        Voc√™ deve lembrar as perguntas feitas para responder quando o usu√°rio perguntar. Deve responder sempre no mesmo idioma que for realizado a pergunta.

        **Diretrizes para suas respostas:**
        - **Clareza**: Sempre procure explicar os conceitos de forma clara e compreens√≠vel.
        - **Detalhamento**: Forne√ßa detalhes suficientes para cobrir o t√≥pico solicitado sem sobrecarregar o usu√°rio com informa√ß√µes excessivas.
        - **Exemplos**: Sempre que poss√≠vel, inclua exemplos pr√°ticos para ilustrar os conceitos.
        - **Interatividade**: Quando criar quizzes, ofere√ßa perguntas que desafiem o conhecimento do usu√°rio e forne√ßa feedback sobre as respostas corretas e incorretas.
        - **Tom**: Mantenha um tom amig√°vel e encorajador, como se voc√™ estivesse conversando diretamente com o usu√°rio.

        Por exemplo, se um usu√°rio pergunta sobre "como funciona a fotoss√≠ntese", sua resposta deve explicar o processo em termos simples, talvez come√ßando com uma breve defini√ß√£o, seguida pelos principais passos da fotoss√≠ntese, e terminando com um exemplo de como as plantas utilizam este processo para crescer. Se solicitado, voc√™ tamb√©m pode criar um pequeno quiz para testar a compreens√£o do usu√°rio sobre o tema.
        """
        
        self.human_template = """
        ####{request}####
        """
        self.system_message_prompt = SystemMessagePromptTemplate.from_template(self.system_template)
        self.human_message_prompt = HumanMessagePromptTemplate.from_template(self.human_template, input_variables=["request"])
        self.chat_prompt = ChatPromptTemplate.from_messages([self.system_message_prompt, self.human_message_prompt])

class Agent:
    def __init__(self, chat_model, verbose=True):
        self.logger = logging.getLogger(__name__)
        if verbose:
            self.logger.setLevel(logging.INFO)
        self.chat_model = chat_model
        self.verbose = verbose

    def get_response(self, request):
        template = GeneralEducationTemplate()
        education_chain = LLMChain(
            llm=self.chat_model,
            prompt=template.chat_prompt,
            verbose=self.verbose,
            output_key='response'
        )
        overall_chain = SequentialChain(
            chains=[education_chain],
            input_variables=["request"],
            output_variables=["response"],
            verbose=self.verbose
        )
        return overall_chain({"request": request}, return_only_outputs=True)

# Inicializando o agente geral
my_agent = Agent(chat)

# Streamlit App
st.set_page_config(page_title="App de Aux√≠lio em Aprendizagem")
st.title("App de Aux√≠lio em Aprendizagem üìö")

# Sidebar for parameters
st.sidebar.header("Par√¢metros ‚öôÔ∏è")
temperature = st.sidebar.slider("Temperatura: Controla a aleatoriedade das respostas geradas pelo modelo!", min_value=0.01, max_value=1.00, value=0.10, step=0.01)
max_length = st.sidebar.slider("Comprimento m√°ximo: Define o comprimento m√°ximo da resposta gerada.", min_value=32, max_value=128, value=120, step=1)

# Clear chat history button
if st.sidebar.button("üóëÔ∏è Clear Chat History"):
    st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# Main chat interface
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = [{"role": "assistant", "content": "Como posso ajudar voc√™ hoje? Pergunte-me qualquer coisa!"}]

# Display chat history
for chat in st.session_state['chat_history']:
    with st.chat_message(chat["role"]):
        st.markdown(chat["content"])

# Input for user message
if prompt := st.chat_input("Digite sua pergunta ou t√≥pico de interesse:"):
    st.session_state['chat_history'].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Adicionar resposta completa ao hist√≥rico de chat
    with st.chat_message("assistant"):
        response = my_agent.get_response(prompt)["response"]
    
        # Add agent response to chat history
        st.session_state['chat_history'].append({"role": "assistant", "content": response})